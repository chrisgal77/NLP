{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
    "import torch.optim as optim\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar, ModelCheckpoint\n",
    "from lightning.pytorch.loggers.tensorboard import TensorBoardLogger\n",
    "from gensim.models import KeyedVectors\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "from lightning.pytorch.tuner import Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def measure_time() -> float:\n",
    "    start = perf_counter()\n",
    "    yield lambda: perf_counter() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLISH_TRANSFORMER_MODEL_NAME = \"dkleczek/bert-base-polish-cased-v1\"\n",
    "DATA_PATH = Path.cwd() / \"data\"\n",
    "MODELS_PATH = Path.cwd() / \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name: str = POLISH_TRANSFORMER_MODEL_NAME, start_training_layer: int = -1, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model, model_out_channels = self._get_transformer(model_name=model_name, start_training_layer=start_training_layer)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=model_out_channels, out_features=1024),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=1024, out_features=num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        pooler_output = self.model(input_ids, attention_mask=attention_mask)[\"pooler_output\"]\n",
    "\n",
    "        return self.classifier(pooler_output)\n",
    "    \n",
    "    def _get_transformer(self, model_name: str, start_training_layer: int):\n",
    "        \"\"\"Get pretrained Transformer model.\n",
    "\n",
    "        Args:\n",
    "            start_training_layer (int): Get number of layer from which model will be unfrozen. Pass -1 if unfreeze none of them.\n",
    "        \"\"\"\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        if start_training_layer == -1:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            return model, model.pooler.dense.out_features\n",
    "\n",
    "        start_training_index = start_training_layer * 16\n",
    "\n",
    "        for param in model.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for idx, param in enumerate(model.encoder.layer.parameters()):\n",
    "            param.requires_grad = False if idx < start_training_index else True\n",
    "\n",
    "        for param in model.pooler.parameters():\n",
    "            param.requires_grad = True if start_training_layer != -1 else False\n",
    "\n",
    "        return model, model.pooler.dense.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransfromerDataset(Dataset):\n",
    "    def __init__(self, data_df: pd.DataFrame, target_column: str, text_column: str, model_name: str = POLISH_TRANSFORMER_MODEL_NAME):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data, self.target = self._prepare_data_to_transformer(\n",
    "            data_df=data_df,\n",
    "            target_column=target_column,\n",
    "            text_column=text_column,\n",
    "            model_name=model_name\n",
    "        )\n",
    "\n",
    "        self.class_mapping = {\n",
    "            class_name: idx for idx, class_name in enumerate((np.unique(self.target)))\n",
    "        }\n",
    "\n",
    "        self.num_classes = max(list(self.class_mapping.values())) + 1\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample_data_input_id = torch.tensor(self.data[\"input_ids\"][index])\n",
    "        sample_data_attention_mask = torch.tensor(self.data[\"attention_mask\"][index])\n",
    "        sample_target = F.one_hot(\n",
    "            torch.tensor(self.class_mapping[self.target[index]]), num_classes=self.num_classes\n",
    "        ).float()\n",
    "\n",
    "        return sample_data_input_id, sample_data_attention_mask, sample_target\n",
    "    \n",
    "    def _prepare_data_to_transformer(\n",
    "        self, data_df: pd.DataFrame, target_column: str, text_column: str, model_name: str = POLISH_TRANSFORMER_MODEL_NAME\n",
    "    ):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        data = tokenizer.batch_encode_plus(\n",
    "            data_df[text_column].tolist(),\n",
    "            max_length = 512,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        target = data_df[target_column].tolist()\n",
    "\n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.target)\n",
    "    \n",
    "    def get_labels(self) -> list[int]:\n",
    "        return [self.class_mapping[label] for label in self.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDatasetModule(L.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.train = TransfromerDataset(\n",
    "            data_df=pd.read_csv(self.hparams.data_root / \"train.csv\"),\n",
    "            target_column=self.hparams.target_column,\n",
    "            text_column=self.hparams.text_column,\n",
    "            model_name=self.hparams.model_name\n",
    "        )\n",
    "        self.test = TransfromerDataset(\n",
    "            data_df=pd.read_csv(self.hparams.data_root / \"test.csv\"),\n",
    "            target_column=self.hparams.target_column,\n",
    "            text_column=self.hparams.text_column,\n",
    "            model_name=self.hparams.model_name\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.hparams.batch_size, shuffle=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.hparams.batch_size, shuffle=False)\n",
    "    \n",
    "    def get_class_weights(self) -> list[float]:\n",
    "        labels = self.train.get_labels()\n",
    "        return torch.tensor(compute_class_weight('balanced', classes=np.unique(labels), y=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModule(L.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = TransformerWrapper(\n",
    "            model_name=self.hparams.model_name,\n",
    "            start_training_layer=self.hparams.start_training_layer,\n",
    "            num_classes=self.hparams.num_classes\n",
    "        )\n",
    "\n",
    "        metrics = MetricCollection([\n",
    "            MulticlassAccuracy(self.hparams.num_classes, average=None),\n",
    "            MulticlassPrecision(self.hparams.num_classes, average=None),\n",
    "            MulticlassRecall(self.hparams.num_classes, average=None),\n",
    "            MulticlassF1Score(self.hparams.num_classes, average=None)\n",
    "        ])\n",
    "        self.metrics = {\n",
    "            \"train\": metrics.clone(prefix='train_'),\n",
    "            \"test\": metrics.clone(prefix='test_')\n",
    "        }\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=self.hparams.class_weights)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        return self.model(input_ids, attention_mask)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_eval(batch, batch_idx, \"train\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_eval(batch, batch_idx, \"test\")\n",
    "\n",
    "    def _shared_eval(self, batch, batch_idx, stage):\n",
    "        input_ids, attention_mask, targets = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "\n",
    "        loss = self.criterion(logits, targets)\n",
    "\n",
    "        self.metrics[stage].update(torch.argmax(logits, -1), torch.argmax(targets, -1))\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        metrics = self.metrics[\"train\"].compute()\n",
    "\n",
    "        for metric_name, values in metrics.items():\n",
    "            for idx, value in enumerate(values):\n",
    "                self.log(f\"{metric_name}_class_{idx}\", value, on_epoch=True)\n",
    "\n",
    "        self.metrics[\"train\"].reset()\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        metrics = self.metrics[\"test\"].compute()\n",
    "\n",
    "        for metric_name, values in metrics.items():\n",
    "            for idx, value in enumerate(values):\n",
    "                self.log(f\"{metric_name}_class_{idx}\", value, on_epoch=True)\n",
    "\n",
    "        self.metrics[\"test\"].reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training\n",
    "\n",
    "## Setup\n",
    "- only classification head\n",
    "- unfreeze last encoder layer + classification head\n",
    "- unfreeze last 2 encoder layers + classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_scores = {}\n",
    "\n",
    "for start_training_layer in [-1, 9, 10]:\n",
    "    datamodule = TransformerDatasetModule(\n",
    "        target_column=\"label\",\n",
    "        text_column=\"preprocessed_text\",\n",
    "        batch_size=128,\n",
    "        model_name=POLISH_TRANSFORMER_MODEL_NAME,\n",
    "        data_root=DATA_PATH\n",
    "    )\n",
    "    datamodule.setup()\n",
    "\n",
    "    model = TransformerModule(\n",
    "        model_name=POLISH_TRANSFORMER_MODEL_NAME,\n",
    "        num_classes=2,\n",
    "        start_training_layer=start_training_layer,\n",
    "        lr=2e-5,\n",
    "        class_weights=datamodule.get_class_weights()\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=30,\n",
    "        accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "        logger=TensorBoardLogger(save_dir=\"logs/\"),\n",
    "        log_every_n_steps=20,\n",
    "    )\n",
    "\n",
    "    tuner = Tuner(trainer)\n",
    "\n",
    "    # tuner.scale_batch_size(\n",
    "    #     model=model,\n",
    "    #     datamodule=datamodule,\n",
    "    #     method=\"fit\"\n",
    "    # )\n",
    "\n",
    "    tuner.lr_find(\n",
    "        model=model,\n",
    "        datamodule=datamodule,\n",
    "        method=\"fit\"\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    transformer_test_scores = trainer.test(model, datamodule=datamodule)\n",
    "\n",
    "    transformer_scores[start_training_layer] = transformer_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256, batch_first=True, num_layers=1, bidirectional=True, dropout=0.2)\n",
    "\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        _, (last_hidden, _) = self.lstm(sequence)\n",
    "\n",
    "        return self.fcn(last_hidden[-1])\n",
    "    \n",
    "    def _get_word2vec(self, model_path: str):\n",
    "        return KeyedVectors.load_word2vec_format(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModule(L.LightningModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = Word2VecWrapper(\n",
    "            num_classes=self.hparams.num_classes\n",
    "        )\n",
    "\n",
    "        metrics = MetricCollection([\n",
    "            MulticlassAccuracy(self.hparams.num_classes, average=None),\n",
    "            MulticlassPrecision(self.hparams.num_classes, average=None),\n",
    "            MulticlassRecall(self.hparams.num_classes, average=None),\n",
    "            MulticlassF1Score(self.hparams.num_classes, average=None)\n",
    "        ])\n",
    "        self.metrics = {\n",
    "            \"train\": metrics.clone(prefix='train_'),\n",
    "            \"test\": metrics.clone(prefix='test_')\n",
    "        }\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=self.hparams.class_weights)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        return self.model(sequence)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_eval(batch, batch_idx, \"train\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._shared_eval(batch, batch_idx, \"test\")\n",
    "\n",
    "    def _shared_eval(self, batch, batch_idx, stage):\n",
    "        sequences, targets = batch\n",
    "        batch_size = targets.shape[0]\n",
    "        logits = self(sequences)\n",
    "\n",
    "        loss = self.criterion(logits, targets)\n",
    "\n",
    "        self.metrics[stage].update(torch.argmax(logits, -1), torch.argmax(targets, -1))\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss, on_epoch=True, on_step=True, batch_size=batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        metrics = self.metrics[\"train\"].compute()\n",
    "\n",
    "        for metric_name, values in metrics.items():\n",
    "            for idx, value in enumerate(values):\n",
    "                self.log(f\"{metric_name}_class_{idx}\", value, on_epoch=True)\n",
    "\n",
    "        self.metrics[\"train\"].reset()\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        metrics = self.metrics[\"test\"].compute()\n",
    "\n",
    "        for metric_name, values in metrics.items():\n",
    "            for idx, value in enumerate(values):\n",
    "                self.log(f\"{metric_name}_class_{idx}\", value, on_epoch=True)\n",
    "\n",
    "        self.metrics[\"test\"].reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, data_df: pd.DataFrame, target_column: str, text_column: str, model_path: str = \"glove_100_3_polish.txt\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.word2vec = KeyedVectors.load_word2vec_format(model_path)\n",
    "\n",
    "        self.data, self.target = self._prepare_data_to_transformer(\n",
    "            data_df=data_df,\n",
    "            target_column=target_column,\n",
    "            text_column=text_column,\n",
    "        )\n",
    "\n",
    "        self.class_mapping = {\n",
    "            class_name: idx for idx, class_name in enumerate((np.unique(self.target)))\n",
    "        }\n",
    "\n",
    "        self.num_classes = max(list(self.class_mapping.values())) + 1\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        sample_data = torch.tensor(self.data[index]).float()\n",
    "\n",
    "        sample_target = F.one_hot(\n",
    "            torch.tensor(self.class_mapping[self.target[index]]), num_classes=self.num_classes\n",
    "        ).float()\n",
    "\n",
    "        return sample_data, sample_target\n",
    "    \n",
    "    def _prepare_data_to_transformer(\n",
    "        self, data_df: pd.DataFrame, target_column: str, text_column: str\n",
    "    ):\n",
    "        data = data_df[text_column].tolist()\n",
    "\n",
    "        data = [\n",
    "            element.split(\" \") for element in data\n",
    "        ]\n",
    "\n",
    "        oov_embedding = np.random.random(self.word2vec.vector_size)\n",
    "\n",
    "        data = [\n",
    "            [\n",
    "                self.word2vec.get_vector(word) if word in self.word2vec.key_to_index else oov_embedding for word  in words\n",
    "            ] for words in data\n",
    "        ]\n",
    "\n",
    "        target = data_df[target_column].tolist()\n",
    "\n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.target)\n",
    "    \n",
    "    def get_labels(self) -> list[int]:\n",
    "        return [self.class_mapping[label] for label in self.target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDatasetModule(L.LightningDataModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.train = LSTMDataset(\n",
    "            data_df=pd.read_csv(self.hparams.data_root / \"train.csv\"),\n",
    "            target_column=self.hparams.target_column,\n",
    "            text_column=self.hparams.text_column,\n",
    "        )\n",
    "        self.test = LSTMDataset(\n",
    "            data_df=pd.read_csv(self.hparams.data_root / \"test.csv\"),\n",
    "            target_column=self.hparams.target_column,\n",
    "            text_column=self.hparams.text_column,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.hparams.batch_size, shuffle=True, collate_fn=self._collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.hparams.batch_size, shuffle=False, collate_fn=self._collate_fn)\n",
    "    \n",
    "    def _collate_fn(self, batch: list[tuple[torch.Tensor, torch.Tensor]]):\n",
    "        sequences, targets = [seq for seq, _ in batch], [target for _, target in batch]\n",
    "        \n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        \n",
    "        padded_seqs = pad_sequence(sequences, batch_first=True)\n",
    "        \n",
    "        packed_seqs = pack_padded_sequence(padded_seqs, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        return packed_seqs, torch.stack(targets)\n",
    "    \n",
    "    def get_class_weights(self) -> list[float]:\n",
    "        labels = self.train.get_labels()\n",
    "        return torch.tensor(compute_class_weight('balanced', classes=np.unique(labels), y=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training\n",
    "\n",
    "## Setup\n",
    "- Word embeddings from GloVe + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = LSTMDatasetModule(\n",
    "    target_column=\"label\",\n",
    "    text_column=\"preprocessed_text\",\n",
    "    batch_size=128,\n",
    "    model_path=MODELS_PATH / \"glove_100_3_polish.txt\",\n",
    "    data_root=DATA_PATH\n",
    ")\n",
    "datamodule.setup()\n",
    "\n",
    "model = LSTMModule(\n",
    "    model_path=MODELS_PATH / \"glove_100_3_polish.txt\",\n",
    "    num_classes=2,\n",
    "    lr=1e-3,\n",
    "    class_weights=datamodule.get_class_weights()\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"cuda\",\n",
    "    callbacks=[\n",
    "        TQDMProgressBar(refresh_rate=20),\n",
    "    ],\n",
    "    logger=TensorBoardLogger(save_dir=\"logs/\"),\n",
    "    log_every_n_steps=20,\n",
    ")\n",
    "\n",
    "tuner = Tuner(trainer)\n",
    "\n",
    "# tuner.scale_batch_size(\n",
    "#     model=model,\n",
    "#     datamodule=datamodule,\n",
    "#     method=\"fit\"\n",
    "# )\n",
    "\n",
    "tuner.lr_find(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    method=\"fit\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=datamodule)\n",
    "lstm_test_scores = trainer.test(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "\n",
    "class FillingMaskDataGenerator:\n",
    "    def __init__(self) -> None:\n",
    "        model = BertForMaskedLM.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
    "        self.nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer, top_k=3)\n",
    "\n",
    "    def get_for_single(self, masked_sentence: str, n: int = 3) -> Iterator[str]:\n",
    "        \"\"\"Create n examples with filled mask\n",
    "\n",
    "        Args:\n",
    "            masked_sentence (str): Sentence with '[MASK]' where to fill\n",
    "            n (int, optional): n examples. Defaults to 3.\n",
    "        \"\"\"\n",
    "        yield from [result[\"sequence\"] for result in self.nlp(masked_sentence)]\n",
    "    \n",
    "    def get_for_iterable(self, masked_sequences: Iterable[str], n: int = 3) -> Iterator[str]:\n",
    "        for masked_sequence in masked_sequences:\n",
    "            yield from self.get_for_single(masked_sentence=masked_sequence, n=n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
